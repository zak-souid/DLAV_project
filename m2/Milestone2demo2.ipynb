{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTZTFn4cfokg"
      },
      "source": [
        "# Introduction \n",
        "\n",
        "This jupyter notebook contains a demo version of the SiamMask tracker. \n",
        "This demo is based on Qiang Wang's (@foolwood on github) implemantation which you can find at this adress : https://github.com/foolwood/SiamMask\n",
        "\n",
        "It is designed to work in Google Colab \n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zak-souid/DLAV_project/main/m2/Milestone2demo2.ipynb)\n",
        "\n",
        "\n",
        "When trying to implement it with our milestone 1 we had issues with python finding the different libraries and did not manage to make it work in time. The issue still puzzles us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy6AWpcefgO4"
      },
      "source": [
        "#### Initial imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEkUd5FIASG6",
        "outputId": "a251ab6d-b5bf-4cb0-8447-549a8008dce2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: got10k in /usr/local/lib/python3.7/dist-packages (0.1.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (3.4.3.18)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.15.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.21.0)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (from got10k) (3.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from got10k) (2.2.3)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from got10k) (1.8.2)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.7/dist-packages (from got10k) (0.1.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire->got10k) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->got10k) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->got10k) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->got10k) (0.11.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from matplotlib->got10k) (2022.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->got10k) (1.4.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "!pip install torch torchvision got10k opencv-python\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from collections import namedtuple\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "from torch.utils.data import DataLoader\n",
        "from got10k.trackers import Tracker\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "# load the files for milestone 2\n",
        "#if not os.path.exists('DLAV_project/siamfc-pytorch/'):\n",
        "#  !git clone --branch milestone2 https://github.com/zak-souid/DLAV_project.git ;\n",
        "\n",
        "import sys\n",
        "# load the files for milestone 1\n",
        "\n",
        "#obj_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "#hand_model = torch.hub.load('ultralytics/yolov5', 'custom', path='DLAV_project/best.pt')\n",
        "\n",
        "import glob\n",
        "from google.colab.patches import cv2_imshow\n",
        "from os.path import exists, join, basename, splitext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KCvXIMyBG4L1"
      },
      "outputs": [],
      "source": [
        "git_repo_url = 'https://github.com/foolwood/SiamMask.git'\n",
        "project_name = splitext(basename(git_repo_url))[0]\n",
        "if not exists(project_name):\n",
        "  # clone and install dependencies\n",
        "  !git clone -q --depth 1 {git_repo_url}\n",
        "  !sed -i \"/torch/d\" {project_name}/requirements.txt\n",
        "  !cd {project_name} && pip install -q -r requirements.txt\n",
        "  !cd {project_name} && bash make.sh\n",
        "  \n",
        "import sys\n",
        "sys.path.append(project_name)\n",
        "sys.path.append(join(project_name, 'experiments', 'siammask_sharp'))\n",
        "import time\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"axes.grid\"] = False\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "from types import SimpleNamespace\n",
        "from custom import Custom\n",
        "from tools.test import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0ga-RHVGvEO",
        "outputId": "732db93b-1910-47f4-b917-6673b2245ed0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2022-06-07 19:10:14,972-rk0-features.py# 66] Current training 0 layers:\n",
            "\t\n",
            "[2022-06-07 19:10:14,976-rk0-features.py# 66] Current training 1 layers:\n",
            "\t\n",
            "[2022-06-07 19:10:15,039-rk0-load_helper.py# 31] load pretrained model from SiamMask/experiments/siammask_sharp/SiamMask_DAVIS.pth\n",
            "[2022-06-07 19:10:15,222-rk0-load_helper.py# 25] remove prefix 'module.'\n",
            "[2022-06-07 19:10:15,227-rk0-load_helper.py# 18] used keys:356\n"
          ]
        }
      ],
      "source": [
        "exp_path = join(project_name, 'experiments/siammask_sharp')\n",
        "pretrained_path1 = join(exp_path, 'SiamMask_DAVIS.pth')\n",
        "pretrained_path2 = join(exp_path, 'SiamMask_VOT.pth')\n",
        "\n",
        "# download pretrained weights\n",
        "if not exists(pretrained_path1):\n",
        "  !cd {exp_path} && wget http://www.robots.ox.ac.uk/~qwang/SiamMask_DAVIS.pth\n",
        "if not exists(pretrained_path2):\n",
        "  !cd {exp_path} && wget http://www.robots.ox.ac.uk/~qwang/SiamMask_VOT.pth\n",
        "    \n",
        "# init SiamMask\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "cfg = load_config(SimpleNamespace(config=join(exp_path, 'config_davis.json')))\n",
        "siammask = Custom(anchors=cfg['anchors'])\n",
        "siammask = load_pretrain(siammask, pretrained_path1)\n",
        "siammask = siammask.eval().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mYmDwjm5m5g"
      },
      "source": [
        "#### Helper functions for the camera in colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "V7sX3BtY-hq3"
      },
      "outputs": [],
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "        js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "        img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "        bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hc9tfB6H-pfS"
      },
      "outputs": [],
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[1].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\" }});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7UkdcYN5siC"
      },
      "source": [
        "#### Functions for Milestone 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zHPdMR3D_PHr"
      },
      "outputs": [],
      "source": [
        "# Some functions used in next cell\n",
        "\n",
        "def obj_detect(img):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "          img: OpenCV BGR image\n",
        "    Returns:\n",
        "          res: pandas dataframe containing all objects detected by the model\n",
        "    \"\"\"\n",
        "    results = obj_model(img)\n",
        "    res = results.pandas().xyxy[0]  # img1 predictions (pandas)\n",
        "    return res\n",
        "\n",
        "def get_persons(objects):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "          objects: pandas dataframe containing different objects\n",
        "    Returns:\n",
        "          persons: pandas dataframe containing only the objects labeled as person\n",
        "    \"\"\"\n",
        "    persons = objects.loc[objects['name'] == \"person\"]\n",
        "    idx_del = []\n",
        "    areas = []\n",
        "\n",
        "    for idx, pers in persons.iterrows():\n",
        "        dx = pers[\"xmax\"]-pers[\"xmin\"]\n",
        "        dy = pers[\"ymax\"]-pers[\"ymin\"]\n",
        "        areas.append(dx*dy)\n",
        "        if(areas[-1] < 10000):\n",
        "            idx_del.append(idx)\n",
        "    persons = persons.drop(idx_del)\n",
        "    persons = persons.reset_index()\n",
        "\n",
        "\n",
        "    return persons, areas\n",
        "\n",
        "def get_personOfInterest(persons, hand, persons_area):  # returns None if rectangles don't intersect\n",
        "    \"\"\"\n",
        "    Params:\n",
        "          persons: pandas dataframe containing only objects labeled as person\n",
        "          hand: pandas dataframe containing only the detected hands (labeled as 5)\n",
        "    Returns:\n",
        "          idx_max: integer corresponding to the index in the persons dataframe that corresponds to the person of interest\n",
        "                   is equal to None if no hand is being detected or the hand does not belongs to any of the detectet persons\n",
        "    \"\"\"\n",
        "    margin = 50 # For numerical inaccuracies\n",
        "    idx_max = 0\n",
        "    area_max = 0\n",
        "    a_min = [-1,-1]\n",
        "    a_max = [-1,-1]\n",
        "    if not hand.empty:\n",
        "        for idx, pers in persons.iterrows():\n",
        "            dx = np.minimum(pers[\"xmax\"], hand[\"xmax\"].iloc[0]) - np.maximum(pers[\"xmin\"], hand[\"xmin\"].iloc[0])\n",
        "            dy = np.minimum(pers[\"ymax\"], hand[\"ymax\"].iloc[0]) - np.maximum(pers[\"ymin\"], hand[\"ymin\"].iloc[0])\n",
        "            if(dx<0 and dy<0):\n",
        "                area = -dx*dy\n",
        "            else:\n",
        "                area = dx*dy\n",
        "            if (area >= area_max - margin and persons_area[idx] >= persons_area[idx_max]):\n",
        "                area_max = area\n",
        "                idx_max = idx\n",
        "                # actual overlapping area\n",
        "                a_min = [int(np.maximum(pers[\"xmin\"], hand[\"xmin\"].iloc[0])),int(np.maximum(pers[\"ymin\"], hand[\"ymin\"].iloc[0]))]\n",
        "                a_max = [int(np.minimum(pers[\"xmax\"], hand[\"xmax\"].iloc[0])),int(np.minimum(pers[\"ymax\"], hand[\"ymax\"].iloc[0]))]\n",
        "\n",
        "    if (area_max>0):\n",
        "        return idx_max, a_min, a_max\n",
        "    else:\n",
        "        return None, a_min, a_max\n",
        "\n",
        "def hand_detect(img):\n",
        "    \"\"\"\n",
        "    Params:\n",
        "          img: OpenCV BGR image\n",
        "    Returns:\n",
        "          res: pandas dataframe containing all objects detected by the model\n",
        "    \"\"\"\n",
        "    results = hand_model(img)\n",
        "    res = results.pandas().xyxy[0]  # img1 predictions (pandas)\n",
        "    return res\n",
        "\n",
        "init = False\n",
        "persOfInterest = False\n",
        "bb_persOfInterest = [0,0,0,0]\n",
        "\n",
        "def detect(img):\n",
        "    global persOfInterest\n",
        "    global bb_persOfInterest\n",
        "    # detect all objects\n",
        "    obj = obj_detect(img)\n",
        "    # keep only the persons\n",
        "    pers, persons_area = get_persons(obj)\n",
        "    # detect hand\n",
        "    hand = hand_detect(img)\n",
        "\n",
        "    # identify person of interest\n",
        "    idx_max, area_min, area_max = get_personOfInterest(pers,hand, persons_area)\n",
        "    \n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "    \n",
        "    # draw bounding box of the hand (if multiple hands, just the first one)\n",
        "    if (not hand.empty) and area_min!=None:\n",
        "        bbox_array = cv2.rectangle(bbox_array,(int(hand[\"xmin\"].iloc[0]),int(hand[\"ymin\"].iloc[0])), (int(hand[\"xmax\"].iloc[0]),int(hand[\"ymax\"].iloc[0])), (0, 0, 255), 2)\n",
        "        bbox_array = cv2.rectangle(bbox_array,tuple(area_min), tuple(area_max), (255, 0, 255), 2)\n",
        "\n",
        "    # draw bounding boxes on overlay\n",
        "    for index, row in pers.iterrows():\n",
        "        start_point = (int(row[\"xmin\"]), int(row[\"ymin\"]))\n",
        "        end_point = (int(row[\"xmax\"]), int(row[\"ymax\"]))\n",
        "        if (index == idx_max):\n",
        "            color = (0, 255, 0)\n",
        "            persOfInterest = True\n",
        "            bb_persOfInterest = [start_point[0], start_point[1], end_point[0]-start_point[0], end_point[1]-start_point[1]]\n",
        "            #print(bb_persOfInterest)\n",
        "        else:\n",
        "            color = (255, 0, 0)\n",
        "        thickness = 2\n",
        "        bbox_array = cv2.rectangle(bbox_array,start_point, end_point, color, thickness)\n",
        "\n",
        "    return bbox_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVOQXcf36IJX"
      },
      "source": [
        "# SIAMMASK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzPPTQgF6Cku"
      },
      "source": [
        "### Video feed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XFbKsE54_c3y"
      },
      "outputs": [],
      "source": [
        "def start_demo():\n",
        "\n",
        "  f = 0\n",
        "# start streaming video from webcam\n",
        "  video_stream()\n",
        "# label for video\n",
        "  label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "  bbox = ''\n",
        "  count = 0 \n",
        "  init_img = 0\n",
        "  box_detect = [220,150,240,330]\n",
        "  maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "  while (True):   \n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "\n",
        "    frame = js_to_image(js_reply[\"img\"])\n",
        "    if f == 0:\n",
        "      # detect a person on the first frame with Mask R-CNN\n",
        "      maskrcnn = maskrcnn.eval()\n",
        "      output = maskrcnn([torchvision.transforms.functional.to_tensor((frame[...,::-1] / 255).astype(np.float32))])[0]\n",
        "\n",
        "      target_pos, target_sz = None, None\n",
        "      for box, label, score in zip(output['boxes'], output['labels'], output['scores']):\n",
        "        # search for a person with atleast 70% probability\n",
        "        if score > 0.7 and label == 1:\n",
        "          box = box.numpy()\n",
        "          x, y = int(box[0]), int(box[1])\n",
        "          w, h = int(box[2]) - x, int(box[3]) - y\n",
        "          target_pos = np.array([x + w / 2, y + h / 2])\n",
        "          target_sz = np.array([w, h])\n",
        "          break\n",
        "      assert target_pos is not None, \"no person found on the first frame!\"\n",
        "      print(\"person found:\", target_pos, target_sz)\n",
        "      #target_pos, target_sz = bbox_persofinterest\n",
        "      # init tracker\n",
        "      state = siamese_init(frame, target_pos, target_sz, siammask, cfg['hp'], device=device)\n",
        "      #state = detect(img)\n",
        "    else:\n",
        "      # track\n",
        "      state = siamese_track(state, frame, mask_enable=True, refine_enable=True, device=device)\n",
        "      location = state['ploygon'].flatten()\n",
        "      mask = state['mask'] > state['p'].seg_thr\n",
        "      \n",
        "      frame[:, :, 2] = (mask > 0) * 255 + (mask == 0) * frame[:, :, 2]\n",
        "      bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "      bbox_array = cv2.polylines(bbox_array, [np.int0(location).reshape((-1, 1, 2))], True, (255, 255, 0), 2)\n",
        "      #cv2_imshow(frame)\n",
        "      bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "      bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "      bbox = bbox_bytes\n",
        "  \n",
        "    f += 1\n",
        "    # only on first 100 frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7KdGSgK8x5W"
      },
      "source": [
        "# Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t28LYmzi5fNm"
      },
      "outputs": [],
      "source": [
        "start_demo()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Milestone2demo2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
